# Stochastic Gradient Descent (SGD) from Scratch

This project implements **Stochastic Gradient Descent (SGD)** in Python from first principles. It explores how SGD updates model parameters incrementally using single training examples, making it computationally more efficient than full-batch gradient descent for large datasets.

## ðŸ”¹ Project Overview
- Implemented SGD to optimize linear models for classification and regression tasks  
- Compared convergence behavior with **full-batch gradient descent**  
- Documented algorithm details directly within the notebook for clarity  
- Explored the impact of **learning rates, momentum, and stochastic shuffling** on optimization performance  

## ðŸ”¹ Key Features
- **From Scratch Implementation**: All update rules coded without ML libraries  
- **Algorithm Explanations**: Step-by-step breakdown of SGD mechanics  
- **Experimentation**: Tested hyperparameters such as learning rate and momentum  
- **Visualization**: Loss and convergence dynamics plotted in real-time  

## ðŸ”¹ Technologies Used
- Python  
- NumPy  
- Jupyter Notebook  
- Matplotlib  

## ðŸ”¹ How to Run
1. Clone this repository:
   ```bash
   git clone https://github.com/rahiqraees/Stochastic-GD.git
   
2. Open the notebook:
   jupyter notebook sgd.ipynb

3. Run all cells to view the algorithm implementation, explanations, and results.

